# Seq2Seq

## Prerequisite

이 글은 순환 신경망과 워드 임베딩에 대한 사전지식을 필요로 합니다.

## Sequence to Sequence

Seq2Seq란 입력 시퀀스를 다른 **도메인**의 출력 시퀀스로 변환하는 순환 신경망 모델입니다. **도메인 변환**의 예시로 **한국어-영어, 음성-텍스트, 긴 문장-요약문장, 질문-대답 등이** 있습니다. 대표적 응용 예시로 번역기와 자동응답 챗봇이 있습니다.

Seq2Seq를 일반적으로 설명하기에 비직관적이므로, 대표적인 예시인 기계번역 예시를 사용해 Seq2Seq를 이론적으로 이해해 보려고 합니다.

### 단순 RNN 시퀀스 출력의 단점

* 나는 어제 사과를 샀다.
* I bought an apple yesterday.

이 두 문장을 번역하기 위해 단순히 RNN을 몇 개 연결해서 사용해 봅시다.

![](../.gitbook/assets/image%20%2810%29.png)

같은 의미를 가지는 다른 언어의 두 문장이 어순, 길이 모두 다릅니다. RNN은 hidden state와 현재 입력을 이용해 출력을 예측하기 때문에 입력과 출력의 문장 길이는 같아야 합니다. 또한 어순이 다르기 때문에 뒤에 있는 단어를 앞으로 옮기는 힘겨운 작업을 해 주어야 합니다. 따라서 이런 방법은 번역 성능을 크게 떨어뜨릴 것입니다.

그럼 이를 어떻게 해결할 수 있을까요?

### 인코더-디코더 구조

단어 하나하나를 보지 말고, 문장 전체를 먼저 이해한 후 번역해 이 문제를 해결할 수 있습니다. 문장을 이해하는 과정을 **인코딩**, 이해한 문장을 다른 언어로 풀어주는 과정을 **디코딩**이라고 할 수 있습니다.

![](../.gitbook/assets/image%20%2811%29.png)

Seq2Seq는 이렇게 인코더와 디코더로 이루어져 있으며, 인코더의 최종 출력은 고정된 차원의 벡터입니다. 이 벡터를 디코더의 초기 hidden state에 입력해 인코더가 이해한 문장의 내용을 번역할 수 있도록 디코더에 전달해 줍니다. **이 벡터는 문장의 맥락을 포함하고 있다 하여 Context Vector\(또는 Thought Vector\)라고 부릅니다.**

인코더와 디코더는 모두 순환 신경망으로 이루어져 있습니다.



## References

[https://wikidocs.net/24996](https://wikidocs.net/24996)  




